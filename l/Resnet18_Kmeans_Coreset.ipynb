{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilaUylWaJyeD",
        "outputId": "4512c6dd-5e82-463b-8bf6-e07ef658ae38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading CIFAR-10 dataset...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Training samples: 50000, Test samples: 10000\n",
            "\n",
            "Setting up the ResNet18 model...\n",
            "Using device: cuda\n",
            "\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "Epoch [1/10], Step [100/782], Loss: 1.5706\n",
            "Epoch [1/10], Step [200/782], Loss: 1.4243\n",
            "Epoch [1/10], Step [300/782], Loss: 1.5141\n",
            "Epoch [1/10], Step [400/782], Loss: 1.2019\n",
            "Epoch [1/10], Step [500/782], Loss: 1.1761\n",
            "Epoch [1/10], Step [600/782], Loss: 1.2501\n",
            "Epoch [1/10], Step [700/782], Loss: 1.1338\n",
            "Epoch [1] finished. Avg Loss: 1.3867. Time: 11.79 sec\n",
            "\n",
            "Epoch [2/10], Step [100/782], Loss: 0.9666\n",
            "Epoch [2/10], Step [200/782], Loss: 1.1940\n",
            "Epoch [2/10], Step [300/782], Loss: 0.9068\n",
            "Epoch [2/10], Step [400/782], Loss: 0.7468\n",
            "Epoch [2/10], Step [500/782], Loss: 1.2252\n",
            "Epoch [2/10], Step [600/782], Loss: 0.8584\n",
            "Epoch [2/10], Step [700/782], Loss: 1.1497\n",
            "Epoch [2] finished. Avg Loss: 0.9922. Time: 10.59 sec\n",
            "\n",
            "Epoch [3/10], Step [100/782], Loss: 1.1069\n",
            "Epoch [3/10], Step [200/782], Loss: 1.0525\n",
            "Epoch [3/10], Step [300/782], Loss: 0.7095\n",
            "Epoch [3/10], Step [400/782], Loss: 1.0212\n",
            "Epoch [3/10], Step [500/782], Loss: 0.9896\n",
            "Epoch [3/10], Step [600/782], Loss: 0.6702\n",
            "Epoch [3/10], Step [700/782], Loss: 0.9070\n",
            "Epoch [3] finished. Avg Loss: 0.8159. Time: 10.78 sec\n",
            "\n",
            "Epoch [4/10], Step [100/782], Loss: 0.6540\n",
            "Epoch [4/10], Step [200/782], Loss: 0.5739\n",
            "Epoch [4/10], Step [300/782], Loss: 0.6233\n",
            "Epoch [4/10], Step [400/782], Loss: 0.5625\n",
            "Epoch [4/10], Step [500/782], Loss: 0.8956\n",
            "Epoch [4/10], Step [600/782], Loss: 1.0626\n",
            "Epoch [4/10], Step [700/782], Loss: 0.7773\n",
            "Epoch [4] finished. Avg Loss: 0.6848. Time: 10.58 sec\n",
            "\n",
            "Epoch [5/10], Step [100/782], Loss: 0.5038\n",
            "Epoch [5/10], Step [200/782], Loss: 0.5236\n",
            "Epoch [5/10], Step [300/782], Loss: 0.6212\n",
            "Epoch [5/10], Step [400/782], Loss: 0.7434\n",
            "Epoch [5/10], Step [500/782], Loss: 0.6056\n",
            "Epoch [5/10], Step [600/782], Loss: 0.7390\n",
            "Epoch [5/10], Step [700/782], Loss: 0.4480\n",
            "Epoch [5] finished. Avg Loss: 0.5847. Time: 10.71 sec\n",
            "\n",
            "Epoch [6/10], Step [100/782], Loss: 0.4423\n",
            "Epoch [6/10], Step [200/782], Loss: 0.2123\n",
            "Epoch [6/10], Step [300/782], Loss: 0.6021\n",
            "Epoch [6/10], Step [400/782], Loss: 0.5966\n",
            "Epoch [6/10], Step [500/782], Loss: 0.5208\n",
            "Epoch [6/10], Step [600/782], Loss: 0.4201\n",
            "Epoch [6/10], Step [700/782], Loss: 0.4760\n",
            "Epoch [6] finished. Avg Loss: 0.4849. Time: 10.75 sec\n",
            "\n",
            "Epoch [7/10], Step [100/782], Loss: 0.4136\n",
            "Epoch [7/10], Step [200/782], Loss: 0.4412\n",
            "Epoch [7/10], Step [300/782], Loss: 0.4124\n",
            "Epoch [7/10], Step [400/782], Loss: 0.4421\n",
            "Epoch [7/10], Step [500/782], Loss: 0.3492\n",
            "Epoch [7/10], Step [600/782], Loss: 0.5247\n",
            "Epoch [7/10], Step [700/782], Loss: 0.4116\n",
            "Epoch [7] finished. Avg Loss: 0.4045. Time: 18.44 sec\n",
            "\n",
            "Epoch [8/10], Step [100/782], Loss: 0.1547\n",
            "Epoch [8/10], Step [200/782], Loss: 0.1769\n",
            "Epoch [8/10], Step [300/782], Loss: 0.4757\n",
            "Epoch [8/10], Step [400/782], Loss: 0.4170\n",
            "Epoch [8/10], Step [500/782], Loss: 0.3111\n",
            "Epoch [8/10], Step [600/782], Loss: 0.3886\n",
            "Epoch [8/10], Step [700/782], Loss: 0.2313\n",
            "Epoch [8] finished. Avg Loss: 0.3229. Time: 20.75 sec\n",
            "\n",
            "Epoch [9/10], Step [100/782], Loss: 0.1616\n",
            "Epoch [9/10], Step [200/782], Loss: 0.1481\n",
            "Epoch [9/10], Step [300/782], Loss: 0.2778\n",
            "Epoch [9/10], Step [400/782], Loss: 0.4389\n",
            "Epoch [9/10], Step [500/782], Loss: 0.2421\n",
            "Epoch [9/10], Step [600/782], Loss: 0.3840\n",
            "Epoch [9/10], Step [700/782], Loss: 0.1182\n",
            "Epoch [9] finished. Avg Loss: 0.2593. Time: 11.32 sec\n",
            "\n",
            "Epoch [10/10], Step [100/782], Loss: 0.1688\n",
            "Epoch [10/10], Step [200/782], Loss: 0.1774\n",
            "Epoch [10/10], Step [300/782], Loss: 0.1994\n",
            "Epoch [10/10], Step [400/782], Loss: 0.0712\n",
            "Epoch [10/10], Step [500/782], Loss: 0.1868\n",
            "Epoch [10/10], Step [600/782], Loss: 0.1825\n",
            "Epoch [10/10], Step [700/782], Loss: 0.1885\n",
            "Epoch [10] finished. Avg Loss: 0.2160. Time: 10.96 sec\n",
            "\n",
            "Evaluating model on test set...\n",
            "\n",
            "Test Accuracy: 76.03%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet18\n",
        "import time\n",
        "\n",
        "# --- 1. Load Dataset with Transforms ---\n",
        "print(\"Loading CIFAR-10 dataset...\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Training set\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Test set\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(f\"Training samples: {len(trainset)}, Test samples: {len(testset)}\\n\")\n",
        "\n",
        "\n",
        "# --- 2. Define Model ---\n",
        "print(\"Setting up the ResNet18 model...\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "model = resnet18(num_classes=10)\n",
        "model = model.to(device)\n",
        "\n",
        "# --- 3. Loss and Optimizer ---\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# --- 4. Train the Model ---\n",
        "epochs = 10\n",
        "print(f\"Starting training for {epochs} epochs...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print every 100 mini-batches\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(trainloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    avg_loss = running_loss / len(trainloader)\n",
        "    print(f\"Epoch [{epoch+1}] finished. Avg Loss: {avg_loss:.4f}. Time: {(end_time - start_time):.2f} sec\\n\")\n",
        "\n",
        "\n",
        "# --- 5. Evaluate the Model ---\n",
        "print(\"Evaluating model on test set...\\n\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA6Vz8vpfTew"
      },
      "source": [
        "# Construct coreset with K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvQSbv1afiLS"
      },
      "source": [
        "Extract features:\n",
        "\n",
        "use pretrained features (e.g., ResNet18 up to avgpool layer) to get more meaningful features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ll1xttckxq0"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG5WqEF6PNjR",
        "outputId": "51f9c50a-2ebf-41cb-e8ea-b8fa543a12c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 158MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted features shape: torch.Size([50000, 512])\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained resnet18 for feature extraction\n",
        "feature_extractor = resnet18(pretrained=True)\n",
        "feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-1])  # Remove FC layer\n",
        "feature_extractor = feature_extractor.to(device)\n",
        "feature_extractor.eval()\n",
        "\n",
        "# Extract features from trainset\n",
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in trainloader:\n",
        "        images = images.to(device)\n",
        "        features = feature_extractor(images)\n",
        "        features = features.view(features.size(0), -1)  # Flatten output\n",
        "        all_features.append(features.cpu())\n",
        "        all_labels.append(labels)\n",
        "\n",
        "all_features = torch.cat(all_features, dim=0)\n",
        "all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "print(f\"Extracted features shape: {all_features.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDE9Gs3Cfx6F"
      },
      "source": [
        "Apply KMeans:\n",
        "\n",
        "Use sklearn.cluster.KMeans to cluster features into k groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V33SxvRfzbz",
        "outputId": "364421d7-5600-4f93-d307-c2ce8593e951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running KMeans clustering with 5000 clusters...\n",
            "KMeans clustering done\n",
            "\n"
          ]
        }
      ],
      "source": [
        "k = 5000\n",
        "print(f\"Running KMeans clustering with {k} clusters...\")\n",
        "\n",
        "kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "cluster_ids = kmeans.fit_predict(all_features.numpy())\n",
        "\n",
        "print(f\"KMeans clustering done\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKc05Q8Lf6gY"
      },
      "source": [
        "Pick nearest points to centroids:\n",
        "\n",
        "Select the closest feature to each centroid — that becomes your coreset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk3uOH3Hf9CW",
        "outputId": "5e366980-8b52-486a-ee0e-79073ec1bb9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coreset size: 5000\n"
          ]
        }
      ],
      "source": [
        "# closest point in each cluster\n",
        "cluster_ids = torch.tensor(cluster_ids)\n",
        "\n",
        "coreset_indices = []\n",
        "centroids = torch.tensor(kmeans.cluster_centers_)\n",
        "for i in range(k):\n",
        "    cluster_points = all_features[cluster_ids == i]\n",
        "    cluster_labels = all_labels[cluster_ids == i]\n",
        "\n",
        "    if len(cluster_points) == 0:\n",
        "        continue\n",
        "\n",
        "    centroid = centroids[i]\n",
        "    distances = torch.norm(cluster_points - centroid, dim=1)\n",
        "    closest_idx = torch.argmin(distances)\n",
        "\n",
        "    original_indices = torch.where(cluster_ids == i)[0]\n",
        "    coreset_indices.append(original_indices[closest_idx].item())\n",
        "\n",
        "print(f\"Coreset size: {len(coreset_indices)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsvVvcdkgDZw"
      },
      "source": [
        "The small Dataset and DataLoader:\n",
        "\n",
        "The new dataset with only selected images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUD4EppUgImm"
      },
      "outputs": [],
      "source": [
        "coreset_trainset = Subset(trainset, coreset_indices)\n",
        "coreset_trainloader = torch.utils.data.DataLoader(coreset_trainset, batch_size=64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYnuqHbsgWrF"
      },
      "source": [
        "Train in new set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCv9EWITgY6j",
        "outputId": "45d6958b-49cf-48fc-da0c-a0fbf477f224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up a fresh ResNet18 model for coreset training...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#New ResNet18 Model\n",
        "print(\"Setting up a fresh ResNet18 model for coreset training...\\n\")\n",
        "\n",
        "model = resnet18(num_classes=10)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5x5dy-2kQR-",
        "outputId": "c3ff38b8-9501-46f4-bb9e-d5b89a775380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training on coreset for 10 epochs...\n",
            "\n",
            "Epoch [1] finished. Avg Loss: 1.8949. Time: 56.46 sec\n",
            "\n",
            "Epoch [2] finished. Avg Loss: 1.5177. Time: 56.31 sec\n",
            "\n",
            "Epoch [3] finished. Avg Loss: 1.3347. Time: 56.46 sec\n",
            "\n",
            "Epoch [4] finished. Avg Loss: 1.1600. Time: 56.59 sec\n",
            "\n",
            "Epoch [5] finished. Avg Loss: 1.0248. Time: 57.51 sec\n",
            "\n",
            "Epoch [6] finished. Avg Loss: 0.8959. Time: 57.33 sec\n",
            "\n",
            "Epoch [7] finished. Avg Loss: 0.8041. Time: 56.22 sec\n",
            "\n",
            "Epoch [8] finished. Avg Loss: 0.5721. Time: 57.11 sec\n",
            "\n",
            "Epoch [9] finished. Avg Loss: 0.5258. Time: 56.65 sec\n",
            "\n",
            "Epoch [10] finished. Avg Loss: 0.4358. Time: 57.52 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Train the Model on Coreset\n",
        "epochs = 10\n",
        "print(f\"Starting training on coreset for {epochs} epochs...\\n\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(coreset_trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(coreset_trainloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    avg_loss = running_loss / len(coreset_trainloader)\n",
        "    print(f\"Epoch [{epoch+1}] finished. Avg Loss: {avg_loss:.4f}. Time: {(end_time - start_time):.2f} sec\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ohwFc8kkTXX",
        "outputId": "5de69ba6-8f5f-40b8-d5c5-b708a9ec9b20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model on test set...\n",
            "\n",
            "Test Accuracy after Coreset Training: 53.05%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Evaluate Model\n",
        "print(\"Evaluating model on test set...\\n\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy after Coreset Training: {100 * correct / total:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf5UqM3xnotv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFfWIL1UnoXa"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju_ygsJNnqRQ",
        "outputId": "817678d5-1cfe-4aa1-f197-c344cc6e0f7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying PCA to reduce feature dimensions...\n",
            "PCA-reduced feature shape: (50000, 100)\n",
            "\n",
            "Clustering PCA features with KMeans...\n",
            "KMeans clustering done\n",
            "\n",
            "Selecting representative images for coreset...\n",
            "Selected 5000 samples for coreset.\n",
            "\n",
            "Setting up a fresh ResNet18 model for coreset training...\n",
            "\n",
            "Starting training on PCA+KMeans coreset for 10 epochs...\n",
            "\n",
            "Epoch [1] finished. Avg Loss: 1.8885. Time: 56.22 sec\n",
            "\n",
            "Epoch [2] finished. Avg Loss: 1.5213. Time: 57.17 sec\n",
            "\n",
            "Epoch [3] finished. Avg Loss: 1.3267. Time: 56.52 sec\n",
            "\n",
            "Epoch [4] finished. Avg Loss: 1.1977. Time: 57.28 sec\n",
            "\n",
            "Epoch [5] finished. Avg Loss: 1.0020. Time: 56.59 sec\n",
            "\n",
            "Epoch [6] finished. Avg Loss: 0.8184. Time: 57.05 sec\n",
            "\n",
            "Epoch [7] finished. Avg Loss: 0.7337. Time: 56.14 sec\n",
            "\n",
            "Epoch [8] finished. Avg Loss: 0.6048. Time: 57.21 sec\n",
            "\n",
            "Epoch [9] finished. Avg Loss: 0.4409. Time: 56.24 sec\n",
            "\n",
            "Epoch [10] finished. Avg Loss: 0.4675. Time: 57.10 sec\n",
            "\n",
            "Evaluating model on test set...\n",
            "\n",
            "Test Accuracy after PCA+KMeans Coreset Training: 50.88%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#PCA to Features\n",
        "print(\"Applying PCA to reduce feature dimensions...\")\n",
        "\n",
        "pca_dim = 100\n",
        "pca = PCA(n_components=pca_dim)\n",
        "features_pca = pca.fit_transform(all_features.numpy())\n",
        "\n",
        "print(f\"PCA-reduced feature shape: {features_pca.shape}\\n\")  # (50000, 100)\n",
        "\n",
        "\n",
        "#KMeans on PCA-reduced Features\n",
        "print(\"Clustering PCA features with KMeans...\")\n",
        "\n",
        "k = 5000  # Coreset size\n",
        "kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "cluster_ids = kmeans.fit_predict(features_pca)\n",
        "\n",
        "print(f\"KMeans clustering done\\n\")\n",
        "\n",
        "\n",
        "#Select Coreset (Closest Points)\n",
        "print(\"Selecting representative images for coreset...\")\n",
        "\n",
        "coreset_indices = []\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "for i in range(k):\n",
        "    idxs_in_cluster = np.where(cluster_ids == i)[0]\n",
        "    if len(idxs_in_cluster) == 0:\n",
        "        continue\n",
        "    cluster_features = features_pca[idxs_in_cluster]\n",
        "    centroid = centroids[i]\n",
        "\n",
        "    distances = np.linalg.norm(cluster_features - centroid, axis=1)\n",
        "    closest_idx = idxs_in_cluster[np.argmin(distances)]\n",
        "    coreset_indices.append(closest_idx)\n",
        "\n",
        "print(f\"Selected {len(coreset_indices)} samples for coreset.\\n\")\n",
        "\n",
        "\n",
        "#Build Coreset DataLoader\n",
        "coreset_trainset = Subset(trainset, coreset_indices)\n",
        "coreset_trainloader = DataLoader(coreset_trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "#New ResNet18 Model\n",
        "print(\"Setting up a fresh ResNet18 model for coreset training...\\n\")\n",
        "\n",
        "model = resnet18(num_classes=10)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "#Train the Model on Coreset\n",
        "epochs = 10\n",
        "print(f\"Starting training on PCA+KMeans coreset for {epochs} epochs...\\n\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(coreset_trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(coreset_trainloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    avg_loss = running_loss / len(coreset_trainloader)\n",
        "    print(f\"Epoch [{epoch+1}] finished. Avg Loss: {avg_loss:.4f}. Time: {(end_time - start_time):.2f} sec\\n\")\n",
        "\n",
        "\n",
        "# Evaluate Model\n",
        "print(\"Evaluating model on test set...\\n\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy after PCA+KMeans Coreset Training: {100 * correct / total:.2f}%\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q19yfcnauTGB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ffcv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
