{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wVc3b6LvwZK",
        "outputId": "f3eda1f0-ab4f-48cb-95b6-dd122230b975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gthampak/miniconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/gthampak/miniconda3/envs/ffcv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "#Load CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  #Resize because ResNet expects 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "#Load Pretrained ResNet18\n",
        "model = resnet18(pretrained=True)\n",
        "model.fc = nn.Identity()\n",
        "model.eval()\n",
        "model.cuda()\n",
        "\n",
        "#Extract features\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in trainloader:\n",
        "        inputs = inputs.cuda()\n",
        "        outputs = model(inputs)\n",
        "        features.append(outputs.cpu())\n",
        "        labels.append(targets)\n",
        "\n",
        "features = torch.cat(features)\n",
        "labels = torch.cat(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hgpQYGEv4C3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "N = features.shape[0]\n",
        "sample_size = int(0.05 * N)  #5% uniform random sample\n",
        "uniform_indices = np.random.choice(N, sample_size, replace=False)\n",
        "uniform_sample = features[uniform_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrEM1QZuv5pn"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k = 10 #or any k choosed for initial centers\n",
        "kmeans = KMeans(n_clusters=k, random_state=0).fit(uniform_sample.numpy())\n",
        "centers = torch.tensor(kmeans.cluster_centers_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiccUMRSv67n"
      },
      "outputs": [],
      "source": [
        "#Compute squared distance to nearest center\n",
        "all_features = features\n",
        "diff = all_features.unsqueeze(1) - centers.unsqueeze(0)\n",
        "dists = torch.norm(diff, dim=2) ** 2  #(N, k)\n",
        "min_dists, _ = torch.min(dists, dim=1)\n",
        "\n",
        "#Define the probabilities\n",
        "epsilon = 1e-6  #small constant\n",
        "sampling_probs = min_dists + epsilon\n",
        "sampling_probs /= sampling_probs.sum()\n",
        "sampling_probs = sampling_probs.numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zskFGluXv8ij"
      },
      "outputs": [],
      "source": [
        "coreset_size = int(0.01 * N)  #final coreset size, say 1% of dataset\n",
        "coreset_indices = np.random.choice(N, coreset_size, replace=True, p=sampling_probs)\n",
        "\n",
        "coreset_features = features[coreset_indices]\n",
        "coreset_labels = labels[coreset_indices]\n",
        "\n",
        "#Assign weights\n",
        "coreset_weights = 1.0 / (sampling_probs[coreset_indices] * coreset_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-WF7f0GwAIJ"
      },
      "outputs": [],
      "source": [
        "#coreset_indices(from sampling)\n",
        "\n",
        "coreset_dataset = torch.utils.data.Subset(trainset, coreset_indices)\n",
        "\n",
        "#New DataLoader for training\n",
        "coreset_loader = DataLoader(coreset_dataset, batch_size=128, shuffle=True, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPlwWlR1wCZP",
        "outputId": "ced04c6c-b26b-4077-8aad-91a2ea7cd9b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "\n",
        "model = resnet18(num_classes=10)  #10 classes for CIFAR-10\n",
        "model.cuda()\n",
        "\n",
        "#model.load_state_dict(torch.load('some_pretrained_model.pth'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OivYaTpWwCwJ"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K12Gau0wEPW",
        "outputId": "e5f401be-8b1b-48af-defd-209935665258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Loss: 2.3366, Acc: 13.00%\n",
            "Epoch 2/50, Loss: 2.2783, Acc: 16.80%\n",
            "Epoch 3/50, Loss: 2.1601, Acc: 21.00%\n",
            "Epoch 4/50, Loss: 2.0704, Acc: 23.60%\n",
            "Epoch 5/50, Loss: 1.9050, Acc: 31.40%\n",
            "Epoch 6/50, Loss: 1.8264, Acc: 36.20%\n",
            "Epoch 7/50, Loss: 1.7149, Acc: 41.80%\n",
            "Epoch 8/50, Loss: 1.6110, Acc: 47.20%\n",
            "Epoch 9/50, Loss: 1.5268, Acc: 50.60%\n",
            "Epoch 10/50, Loss: 1.3861, Acc: 54.60%\n",
            "Epoch 11/50, Loss: 1.3033, Acc: 63.60%\n",
            "Epoch 12/50, Loss: 1.1765, Acc: 65.60%\n",
            "Epoch 13/50, Loss: 1.0264, Acc: 72.00%\n",
            "Epoch 14/50, Loss: 0.8985, Acc: 76.80%\n",
            "Epoch 15/50, Loss: 0.7957, Acc: 80.80%\n",
            "Epoch 16/50, Loss: 0.6522, Acc: 86.60%\n",
            "Epoch 17/50, Loss: 0.5416, Acc: 88.00%\n",
            "Epoch 18/50, Loss: 0.4048, Acc: 94.40%\n",
            "Epoch 19/50, Loss: 0.3292, Acc: 96.80%\n",
            "Epoch 20/50, Loss: 0.2602, Acc: 96.80%\n",
            "Epoch 21/50, Loss: 0.1790, Acc: 98.80%\n",
            "Epoch 22/50, Loss: 0.1367, Acc: 99.80%\n",
            "Epoch 23/50, Loss: 0.0823, Acc: 99.80%\n",
            "Epoch 24/50, Loss: 0.0622, Acc: 100.00%\n",
            "Epoch 25/50, Loss: 0.0417, Acc: 100.00%\n",
            "Epoch 26/50, Loss: 0.0329, Acc: 100.00%\n",
            "Epoch 27/50, Loss: 0.0336, Acc: 100.00%\n",
            "Epoch 28/50, Loss: 0.0198, Acc: 100.00%\n",
            "Epoch 29/50, Loss: 0.0164, Acc: 100.00%\n",
            "Epoch 30/50, Loss: 0.0124, Acc: 100.00%\n",
            "Epoch 31/50, Loss: 0.0132, Acc: 100.00%\n",
            "Epoch 32/50, Loss: 0.0137, Acc: 100.00%\n",
            "Epoch 33/50, Loss: 0.0117, Acc: 100.00%\n",
            "Epoch 34/50, Loss: 0.0114, Acc: 100.00%\n",
            "Epoch 35/50, Loss: 0.0102, Acc: 100.00%\n",
            "Epoch 36/50, Loss: 0.0096, Acc: 100.00%\n",
            "Epoch 37/50, Loss: 0.0099, Acc: 100.00%\n",
            "Epoch 38/50, Loss: 0.0092, Acc: 100.00%\n",
            "Epoch 39/50, Loss: 0.0103, Acc: 100.00%\n",
            "Epoch 40/50, Loss: 0.0114, Acc: 100.00%\n",
            "Epoch 41/50, Loss: 0.0099, Acc: 100.00%\n",
            "Epoch 42/50, Loss: 0.0090, Acc: 100.00%\n",
            "Epoch 43/50, Loss: 0.0099, Acc: 100.00%\n",
            "Epoch 44/50, Loss: 0.0094, Acc: 100.00%\n",
            "Epoch 45/50, Loss: 0.0087, Acc: 100.00%\n",
            "Epoch 46/50, Loss: 0.0092, Acc: 100.00%\n",
            "Epoch 47/50, Loss: 0.0148, Acc: 100.00%\n",
            "Epoch 48/50, Loss: 0.0085, Acc: 100.00%\n",
            "Epoch 49/50, Loss: 0.0086, Acc: 100.00%\n",
            "Epoch 50/50, Loss: 0.0094, Acc: 100.00%\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in coreset_loader:\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/total:.4f}, Acc: {100.*correct/total:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7FPMbp4xw2X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqRU7eMHxw_D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIRkHY39xxEq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xso5VAyHxTIS",
        "outputId": "861fa60b-fe1c-45fb-c878-802e76502b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted feature shape: torch.Size([50000, 512])\n",
            "Constructing lightweight coreset...\n",
            "Coreset size: 500\n",
            "Training ResNet18 on coreset...\n",
            "Epoch [1/50] Loss: 2.3335, Acc: 13.20%\n",
            "Epoch [2/50] Loss: 2.2313, Acc: 17.80%\n",
            "Epoch [3/50] Loss: 2.1276, Acc: 19.00%\n",
            "Epoch [4/50] Loss: 1.9781, Acc: 30.00%\n",
            "Epoch [5/50] Loss: 1.8444, Acc: 33.40%\n",
            "Epoch [6/50] Loss: 1.7631, Acc: 35.60%\n",
            "Epoch [7/50] Loss: 1.6586, Acc: 43.20%\n",
            "Epoch [8/50] Loss: 1.5667, Acc: 42.80%\n",
            "Epoch [9/50] Loss: 1.4684, Acc: 50.20%\n",
            "Epoch [10/50] Loss: 1.3276, Acc: 56.60%\n",
            "Epoch [11/50] Loss: 1.2049, Acc: 60.20%\n",
            "Epoch [12/50] Loss: 1.0756, Acc: 67.60%\n",
            "Epoch [13/50] Loss: 0.9331, Acc: 70.40%\n",
            "Epoch [14/50] Loss: 0.8186, Acc: 75.40%\n",
            "Epoch [15/50] Loss: 0.7135, Acc: 80.60%\n",
            "Epoch [16/50] Loss: 0.5886, Acc: 86.40%\n",
            "Epoch [17/50] Loss: 0.5162, Acc: 89.20%\n",
            "Epoch [18/50] Loss: 0.3679, Acc: 93.80%\n",
            "Epoch [19/50] Loss: 0.2954, Acc: 97.00%\n",
            "Epoch [20/50] Loss: 0.2447, Acc: 96.00%\n",
            "Epoch [21/50] Loss: 0.1591, Acc: 99.60%\n",
            "Epoch [22/50] Loss: 0.1313, Acc: 99.00%\n",
            "Epoch [23/50] Loss: 0.0982, Acc: 99.60%\n",
            "Epoch [24/50] Loss: 0.1026, Acc: 99.40%\n",
            "Epoch [25/50] Loss: 0.0757, Acc: 99.20%\n",
            "Epoch [26/50] Loss: 0.0752, Acc: 98.80%\n",
            "Epoch [27/50] Loss: 0.0498, Acc: 99.20%\n",
            "Epoch [28/50] Loss: 0.0605, Acc: 99.20%\n",
            "Epoch [29/50] Loss: 0.0493, Acc: 99.20%\n",
            "Epoch [30/50] Loss: 0.0398, Acc: 99.40%\n",
            "Epoch [31/50] Loss: 0.0302, Acc: 99.80%\n",
            "Epoch [32/50] Loss: 0.0247, Acc: 100.00%\n",
            "Epoch [33/50] Loss: 0.0170, Acc: 100.00%\n",
            "Epoch [34/50] Loss: 0.0175, Acc: 100.00%\n",
            "Epoch [35/50] Loss: 0.0154, Acc: 100.00%\n",
            "Epoch [36/50] Loss: 0.0186, Acc: 100.00%\n",
            "Epoch [37/50] Loss: 0.0125, Acc: 100.00%\n",
            "Epoch [38/50] Loss: 0.0132, Acc: 100.00%\n",
            "Epoch [39/50] Loss: 0.0112, Acc: 100.00%\n",
            "Epoch [40/50] Loss: 0.0121, Acc: 100.00%\n",
            "Epoch [41/50] Loss: 0.0146, Acc: 100.00%\n",
            "Epoch [42/50] Loss: 0.0121, Acc: 100.00%\n",
            "Epoch [43/50] Loss: 0.0114, Acc: 100.00%\n",
            "Epoch [44/50] Loss: 0.0118, Acc: 100.00%\n",
            "Epoch [45/50] Loss: 0.0113, Acc: 100.00%\n",
            "Epoch [46/50] Loss: 0.0132, Acc: 100.00%\n",
            "Epoch [47/50] Loss: 0.0100, Acc: 100.00%\n",
            "Epoch [48/50] Loss: 0.0105, Acc: 100.00%\n",
            "Epoch [49/50] Loss: 0.0137, Acc: 100.00%\n",
            "Epoch [50/50] Loss: 0.0108, Acc: 100.00%\n",
            "Evaluating on CIFAR-10 test set...\n",
            "Test Accuracy: 35.82%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load CIFAR-10 Dataset\n",
        "# -------------------------------\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize to match ResNet input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader_full = DataLoader(trainset, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Feature Extraction (ResNet18)\n",
        "# -------------------------------\n",
        "print(\"Extracting features...\")\n",
        "\n",
        "model_feature = resnet18(pretrained=True)\n",
        "model_feature.fc = nn.Identity()  # remove last layer\n",
        "model_feature = model_feature.to(device)\n",
        "model_feature.eval()\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in trainloader_full:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model_feature(inputs)\n",
        "        features.append(outputs.cpu())\n",
        "        labels.append(targets)\n",
        "\n",
        "features = torch.cat(features)  # shape: (50000, 512)\n",
        "labels = torch.cat(labels)\n",
        "\n",
        "print(f\"Extracted feature shape: {features.shape}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Lightweight Coreset Construction\n",
        "# -------------------------------\n",
        "print(\"Constructing lightweight coreset...\")\n",
        "\n",
        "N = features.shape[0]\n",
        "uniform_sample_size = int(0.05 * N)  # 5% uniform sample\n",
        "coreset_final_size = int(0.01 * N)   # 1% final coreset size\n",
        "\n",
        "# Step 1: Uniform random sample\n",
        "uniform_indices = np.random.choice(N, uniform_sample_size, replace=False)\n",
        "uniform_sample = features[uniform_indices]\n",
        "\n",
        "# Step 2: Rough clustering\n",
        "k = 10  # number of clusters\n",
        "kmeans = KMeans(n_clusters=k, random_state=0).fit(uniform_sample.numpy())\n",
        "centers = torch.tensor(kmeans.cluster_centers_)\n",
        "\n",
        "# Step 3: Compute sampling probabilities\n",
        "diff = features.unsqueeze(1) - centers.unsqueeze(0)  # (N, k, 512)\n",
        "dists = torch.norm(diff, dim=2) ** 2\n",
        "min_dists, _ = torch.min(dists, dim=1)  # (N,)\n",
        "\n",
        "epsilon = 1e-6\n",
        "sampling_probs = min_dists + epsilon\n",
        "sampling_probs /= sampling_probs.sum()\n",
        "sampling_probs = sampling_probs.numpy()\n",
        "\n",
        "# Step 4: Importance sampling\n",
        "coreset_indices = np.random.choice(N, coreset_final_size, replace=True, p=sampling_probs)\n",
        "coreset_features = features[coreset_indices]\n",
        "coreset_labels = labels[coreset_indices]\n",
        "\n",
        "print(f\"Coreset size: {len(coreset_indices)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Coreset DataLoader\n",
        "# -------------------------------\n",
        "coreset_dataset = Subset(trainset, coreset_indices)\n",
        "coreset_loader = DataLoader(coreset_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Train ResNet18 on Coreset\n",
        "# -------------------------------\n",
        "print(\"Training ResNet18 on coreset...\")\n",
        "\n",
        "model = resnet18(num_classes=10)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in coreset_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {running_loss/total:.4f}, Acc: {100.*correct/total:.2f}%\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Evaluate on CIFAR-10 Test Set\n",
        "# -------------------------------\n",
        "print(\"Evaluating on CIFAR-10 test set...\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in testloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100.*correct/total:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVDIHy_LxvmX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ffcv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
