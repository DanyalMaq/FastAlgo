{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "https://github.com/townblack/pytorch-cifar10-resnet18"
      ],
      "metadata": {
        "id": "5zxCCn26RZfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3ySVpuqRFjM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, inchannel, outchannel, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.left = nn.Sequential(\n",
        "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(outchannel),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(outchannel)\n",
        "        )\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or inchannel != outchannel:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(outchannel)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.left(x)\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, ResidualBlock, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inchannel = 64\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.layer1 = self.make_layer(ResidualBlock, 64,  2, stride=1)\n",
        "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
        "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n",
        "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, block, channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)   #strides=[1,1]\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.inchannel, channels, stride))\n",
        "            self.inchannel = channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "\n",
        "    return ResNet(ResidualBlock)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import argparse\n",
        "from resnet import ResNet18\n",
        "import time\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "'''\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr = args.lr * (0.1 ** (epoch // 30))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "'''\n",
        "# Define whether to use GPU\n",
        "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Argument settings to allow command-line input manually, making the style similar to Linux CLI\n",
        "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "parser.add_argument('--outf', default='./model/', help='folder to output images and model checkpoints') # Path to save output results\n",
        "parser.add_argument('--net', default='./model/Resnet18.pth', help=\"path to net (to continue training)\")  # Path to the model for resuming training\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Hyperparameter settings\n",
        "EPOCH = 240\n",
        "BATCH_SIZE = 128\n",
        "LR = 0.001        #learning rate\n",
        "Milestones=[135,185]\n",
        "Debug=False\n",
        "\n",
        "# Prepare and preprocess the dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),  # First pad the image with zeros around, then randomly crop it to 32x32\n",
        "    transforms.RandomHorizontalFlip(), # Random horizontal flip with 50% chance\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), # Mean and std used for normalizing each R, G, B channel\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data/cifar-10-python', train=True, download=False, transform=transform_train) # Training dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)   # Generate batches for training, shuffle data when forming batches\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data/cifar-10-python', train=False, download=False, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "# Cifar-10's label\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Model definition - ResNet\n",
        "net = ResNet18().to(device)\n",
        "\n",
        "# Define loss function and optimization method\n",
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss function, commonly used for multi-class classification\n",
        "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4) # Optimization method is mini-batch momentum-SGD with L2 regularization (weight decay)\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=Milestones,gamma = 0.1)\n",
        "\n",
        "writer=SummaryWriter(\"./logs\")\n",
        "\n",
        "# train\n",
        "if __name__ == \"__main__\":\n",
        "    best_acc = 85  #2 initialize best test accuracy\n",
        "    print(\"Start Training, Resnet-18!\")  # Define number of epochs to iterate over the dataset\n",
        "    with open(\"acc.txt\", \"w\") as f:\n",
        "        with open(\"log.txt\", \"w\")as f2:\n",
        "            for epoch in range(EPOCH):\n",
        "                train_loss=0.0\n",
        "                train_accu=0.0\n",
        "                val_loss=0.0\n",
        "                val_accu=0.0\n",
        "\n",
        "                scheduler.step()\n",
        "                #print(type(optimizer.param_groups[0]))\n",
        "                #print(optimizer.param_groups[0][\"lr\"])\n",
        "                #print('\\nEpoch: %d' % (epoch + 1))\n",
        "                net.train()\n",
        "                sum_loss = 0.0\n",
        "                correct = 0.0\n",
        "                total = 0\n",
        "                begin=time.time()\n",
        "                for i, data in enumerate(trainloader, 0):\n",
        "                    # prepare data\n",
        "                    length = len(trainloader)\n",
        "                    inputs, labels = data\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward + backward\n",
        "                    outputs = net(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # Print loss and accuracy after training each batch\n",
        "                    sum_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += predicted.eq(labels.data).cpu().sum()\n",
        "                    if Debug:\n",
        "                        print(\"[Epoch:{}/{}, Batch:{}/{}] Loss: {:.3f} | Acc: {:.3f}%\".format(epoch+1,EPOCH,i+1,int(trainset.__len__()/BATCH_SIZE),sum_loss/(i+1),100.*correct/total))\n",
        "\n",
        "                    f2.write(\"[Epoch:{}/{}, Batch:{}/{}] Loss: {:.3f} | Acc: {:.3f}%\".format(epoch+1,EPOCH,i+1,int(trainset.__len__()/BATCH_SIZE),sum_loss/(i+1),100.*correct/total))\n",
        "                    f2.write('\\n')\n",
        "                    f2.flush()\n",
        "\n",
        "                train_loss=sum_loss/int(trainset.__len__()/BATCH_SIZE)\n",
        "                train_accu=100.*correct/total\n",
        "\n",
        "               # Evaluate accuracy after each epoch\n",
        "                with torch.no_grad():\n",
        "                    sum_loss = 0.0\n",
        "                    correct = 0.0\n",
        "                    total = 0\n",
        "                    for data in testloader:\n",
        "                        net.eval()\n",
        "                        images, labels = data\n",
        "                        images, labels = images.to(device), labels.to(device)\n",
        "                        outputs = net(images)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        sum_loss += loss.item()\n",
        "                       # Get the class with the highest score (index of outputs.data)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += (predicted == labels).sum()\n",
        "\n",
        "                val_loss=sum_loss/int(testset.__len__()/BATCH_SIZE)\n",
        "                val_accu = 100.*correct/total\n",
        "                end=time.time()\n",
        "                print(\"[Epoch:{}/{}] Train Loss: {:.3f} | Train Acc: {:.3f}% Test Loss: {:.3f} | Test Acc: {:.3f}% Cost time:{:.2f}min\".format(epoch+1,EPOCH,train_loss,train_accu,val_loss,val_accu,(end-begin)/60.0))\n",
        "\n",
        "                writer.add_scalar(\"Loss/train\",train_loss,epoch)\n",
        "                writer.add_scalar(\"Loss/val\",val_loss,epoch)\n",
        "                writer.add_scalar(\"Accu/train\",train_accu,epoch)\n",
        "                writer.add_scalar(\"Accu/val\",val_accu,epoch)\n",
        "                writer.add_scalar(\"Learning rate\",optimizer.param_groups[0][\"lr\"],epoch)\n",
        "\n",
        "                # Write each evaluation result to acc.txt in real time\n",
        "                #print('Saving model......')\n",
        "                torch.save(net.state_dict(), '%s/net_%03d.pth' % (args.outf,epoch + 1))\n",
        "                f.write(\"EPOCH=%03d,Accuracy= %.3f%%\" % (epoch+1,val_accu))\n",
        "                f.write('\\n')\n",
        "                f.flush()\n",
        "                # Record the best test accuracy and save to best_acc.txt\n",
        "                if val_accu > best_acc:\n",
        "                    f3 = open(\"best_acc.txt\", \"w\")\n",
        "                    f3.write(\"EPOCH=%d,best_acc= %.3f%%\" % (epoch+1,val_accu))\n",
        "                    f3.close()\n",
        "                    best_acc = val_accu\n",
        "\n",
        "            print(\"Training Finished, TotalEPOCH=%d\" % EPOCH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "zD4Jb3NhRfR8",
        "outputId": "3af08319-a822-411f-c6de-ec5bdf64e1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'resnet'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-184f6008451f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mresnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResNet18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'resnet'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9bv4sHMSsMW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}